import streamlit as st

def check_password():
    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False

    if not st.session_state.authenticated:
        st.title("ğŸ”’ ë©ì‹¤ ì „ìš© í˜ì´ì§€")
        pwd = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")

        if pwd:
            if pwd == st.secrets["APP_PASSWORD"]:
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
        return False

    return True


if not check_password():
    st.stop()

import io
import hashlib
import importlib.util
import pandas as pd
import streamlit as st
from xml.etree import ElementTree as ET

# =========================
# Page
# =========================
st.title("MPPT / ì¸ë²„í„° / ê¸°ìƒê´€ì¸¡ ë³´ê³ ì„œ í•©ì„±ê¸°")
st.caption("ì—¬ëŸ¬ íŒŒì¼ì„ ì—…ë¡œë“œí•œ ë’¤ [í•©ì„± ì‹¤í–‰]ì„ ëˆ„ë¥´ë©´ ì¤‘ë³µ ì œê±° í›„ datetime ê¸°ì¤€ìœ¼ë¡œ í†µí•©ë³¸ì„ ìƒì„±í•©ë‹ˆë‹¤. (ì‹œê°„ ë‹¨ìœ„ ê³ ì •)")

# =========================
# Dependency helpers
# =========================
def has_pkg(name: str) -> bool:
    return importlib.util.find_spec(name) is not None

# =========================
# SpreadsheetML(XML) parser (Excel 2003 XML ëŒ€ì‘)
# =========================
def _strip_ns(tag: str) -> str:
    return tag.split("}", 1)[1] if "}" in tag else tag

def parse_spreadsheetml_xml(xml_bytes: bytes) -> pd.DataFrame:
    root = ET.fromstring(xml_bytes)

    ws = None
    for node in root.iter():
        if _strip_ns(node.tag).lower() == "worksheet":
            ws = node
            break
    if ws is None:
        raise ValueError("XMLì—ì„œ Worksheetë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    table = None
    for node in ws.iter():
        if _strip_ns(node.tag).lower() == "table":
            table = node
            break
    if table is None:
        raise ValueError("XMLì—ì„œ Tableì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    def get_attr_local(elem, local_name: str):
        for k, v in elem.attrib.items():
            if _strip_ns(k).lower() == local_name.lower():
                return v
        return None

    rows_out = []
    max_cols = 0

    for row in [n for n in table if _strip_ns(n.tag).lower() == "row"]:
        cur = []
        col_pos = 1

        for cell in [n for n in row if _strip_ns(n.tag).lower() == "cell"]:
            idx = get_attr_local(cell, "Index")
            if idx:
                idx = int(idx)
                while col_pos < idx:
                    cur.append(None)
                    col_pos += 1

            data_node = None
            for n in cell:
                if _strip_ns(n.tag).lower() == "data":
                    data_node = n
                    break
            val = data_node.text if data_node is not None else None
            cur.append(val)
            col_pos += 1

            merge_across = get_attr_local(cell, "MergeAcross")
            if merge_across:
                ma = int(merge_across)
                for _ in range(ma):
                    cur.append(None)
                    col_pos += 1

        max_cols = max(max_cols, len(cur))
        rows_out.append(cur)

    for r in rows_out:
        if len(r) < max_cols:
            r.extend([None] * (max_cols - len(r)))

    df = pd.DataFrame(rows_out)
    df.columns = list(range(df.shape[1]))
    return df

# =========================
# Universal reader (ë‚´ìš© ê¸°ë°˜ ìë™ íŒë³„)
# =========================
def read_report_table(uploaded_file, header=None) -> pd.DataFrame:
    uploaded_file.seek(0)
    head = uploaded_file.read(16)
    uploaded_file.seek(0)

    uploaded_file.seek(0)
    sniff = uploaded_file.read(4096)
    uploaded_file.seek(0)

    sniff_lower = (sniff or b"").lower()
    is_zip = head.startswith(b"PK")
    is_html = (b"<html" in sniff_lower) or (b"<table" in sniff_lower)
    is_xml = (b"<?xml" in sniff_lower[:600]) or (b"<workbook" in sniff_lower[:1200])

    if is_zip:
        if not has_pkg("openpyxl"):
            raise RuntimeError("ì´ íŒŒì¼ì€ .xlsx í˜•ì‹ì…ë‹ˆë‹¤. openpyxl ì„¤ì¹˜: pip install openpyxl")
        return pd.read_excel(uploaded_file, engine="openpyxl", header=header)

    if is_html:
        uploaded_file.seek(0)
        raw = uploaded_file.read()
        uploaded_file.seek(0)
        text = raw.decode("utf-8", errors="ignore")
        tables = pd.read_html(text)
        if not tables:
            raise ValueError("HTML í…Œì´ë¸”ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        df0 = tables[0]
        df0.columns = list(range(df0.shape[1]))
        return df0

    if is_xml:
        uploaded_file.seek(0)
        xml_bytes = uploaded_file.read()
        uploaded_file.seek(0)
        return parse_spreadsheetml_xml(xml_bytes)

    if not has_pkg("xlrd"):
        raise RuntimeError("'.xls' íŒŒì¼ì„ ì½ìœ¼ë ¤ë©´ xlrd í•„ìš”: pip install xlrd==2.0.1")
    return pd.read_excel(uploaded_file, engine="xlrd", header=header)

# =========================
# Utilities
# =========================
def file_key(file_obj) -> str:
    file_obj.seek(0)
    b = file_obj.read()
    file_obj.seek(0)
    h = hashlib.md5(b).hexdigest()
    return f"{file_obj.name}__{len(b)}__{h}"

def extract_datetime_block(df: pd.DataFrame, dt_col: int = 0) -> pd.DataFrame:
    """
    - 0ì—´ì—ì„œ datetime ë³€í™˜ ê°€ëŠ¥í•œ êµ¬ê°„ë§Œ ì¶”ì¶œ
    - ìµœëŒ€/ìµœì†Œ/í‰ê·  ê°™ì€ ìš”ì•½í–‰ì€ ë³´í†µ datetime ë³€í™˜ì´ ì•ˆ ë˜ë¯€ë¡œ ìë™ ì œì™¸
    """
    if df is None or df.empty:
        raise ValueError("ë¹ˆ ë°ì´í„°ì…ë‹ˆë‹¤.")

    dt = pd.to_datetime(df[dt_col], errors="coerce")
    start = dt.first_valid_index()
    end = dt.last_valid_index()

    if start is None or end is None:
        raise ValueError("datetime ì»¬ëŸ¼(0ì—´)ì—ì„œ ë‚ ì§œ/ì‹œê°„ ë°ì´í„° ì‹œì‘í–‰ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    out = df.loc[start:end].copy().reset_index(drop=True)

    out_dt = pd.to_datetime(out[dt_col], errors="coerce")
    out = out[out_dt.notna()].copy().reset_index(drop=True)
    out.insert(0, "_dt_parsed_", pd.to_datetime(out[dt_col], errors="coerce"))
    return out

def detect_datetime_range(df: pd.DataFrame):
    if df is None or df.empty or "datetime" not in df.columns:
        return None, None
    dt = pd.to_datetime(df["datetime"], errors="coerce").dropna()
    if dt.empty:
        return None, None
    return dt.min(), dt.max()

def to_excel_bytes(df: pd.DataFrame, sheet_name="Merged"):
    out = io.BytesIO()
    if not has_pkg("openpyxl"):
        raise RuntimeError("ì—‘ì…€ë¡œ ì €ì¥í•˜ë ¤ë©´ openpyxlì´ í•„ìš”í•©ë‹ˆë‹¤: pip install openpyxl")
    with pd.ExcelWriter(out, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name=sheet_name)
    out.seek(0)
    return out

def dedup_on_datetime(df: pd.DataFrame) -> pd.DataFrame:
    """datetime ì¤‘ë³µ ì œê±°: ì‹œê°„ ì •ë ¬ í›„ ê°™ì€ datetimeì´ë©´ ë§ˆì§€ë§‰ê°’ ìš°ì„ """
    if df is None or df.empty:
        return df
    x = df.copy()
    x["datetime"] = pd.to_datetime(x["datetime"], errors="coerce")
    x = x.dropna(subset=["datetime"])
    x = x.sort_values("datetime")
    x = x.drop_duplicates(subset=["datetime"], keep="last")
    return x.reset_index(drop=True)

def resample_hourly_mean(df: pd.DataFrame) -> pd.DataFrame:
    """ì‹œê°„(H) ë‹¨ìœ„ ê³ ì •: ë¶„ ë‹¨ìœ„ê°€ ì„ì—¬ ìˆìœ¼ë©´ ì‹œê°„ í‰ê· ìœ¼ë¡œ ì •ê·œí™”"""
    if df is None or df.empty:
        return df
    x = df.copy()
    x["datetime"] = pd.to_datetime(x["datetime"], errors="coerce")
    x = x.dropna(subset=["datetime"]).set_index("datetime").sort_index()
    x = x.resample("H").mean(numeric_only=True).reset_index()
    return x

def prefix_df(df: pd.DataFrame, prefix: str) -> pd.DataFrame:
    if df is None:
        return None
    cols = []
    for c in df.columns:
        cols.append("datetime" if c == "datetime" else f"{prefix}{c}")
    out = df.copy()
    out.columns = cols
    return out

def fill_missing_hours_and_zero(df: pd.DataFrame) -> pd.DataFrame:
    """
    ê³µë°± ê¸°ê°„(ëˆ„ë½ëœ ì‹œê°„í–‰)ì„ ìƒì„±í•˜ê³ , ìˆ˜ì¹˜í˜• ê²°ì¸¡(NaN)ì„ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
    """
    if df is None or df.empty:
        return df

    x = df.copy()
    x["datetime"] = pd.to_datetime(x["datetime"], errors="coerce")
    x = x.dropna(subset=["datetime"]).sort_values("datetime").reset_index(drop=True)

    s = x["datetime"].min().floor("H")
    e = x["datetime"].max().floor("H")
    full = pd.DataFrame({"datetime": pd.date_range(s, e, freq="H")})

    out = full.merge(x, on="datetime", how="left")

    for c in [c for c in out.columns if c != "datetime"]:
        out[c] = pd.to_numeric(out[c], errors="ignore")

    num_cols = out.select_dtypes(include=["number"]).columns.tolist()
    out[num_cols] = out[num_cols].fillna(0)
    return out

# =========================
# Loaders
# =========================
def load_mppt(uploaded_file) -> pd.DataFrame:
    raw = read_report_table(uploaded_file, header=None)
    blk = extract_datetime_block(raw, dt_col=0)

    data = pd.DataFrame()
    data["datetime"] = blk["_dt_parsed_"]

    mapping = [
        ("mppt1_v", 2),
        ("mppt1_a", 3),
        ("mppt2_v", 4),
        ("mppt2_a", 5),
    ]
    for name, idx in mapping:
        if idx < blk.shape[1]:
            data[name] = pd.to_numeric(blk.iloc[:, idx], errors="coerce")

    return data.dropna(subset=["datetime"]).reset_index(drop=True)

def load_inverter(uploaded_file) -> pd.DataFrame:
    raw = read_report_table(uploaded_file, header=None)
    blk = extract_datetime_block(raw, dt_col=0)

    data = pd.DataFrame()
    data["datetime"] = blk["_dt_parsed_"]

    cols_expected = [
        "pv_v", "pv_a", "pv_kw",
        "rs_rn_v", "st_sn_v", "tr_tn_v",
        "freq_hz", "r_a", "s_a", "t_a",
        "inv_kw", "energy_kwh"
    ]
    for i, name in enumerate(cols_expected, start=2):
        if i < blk.shape[1]:
            data[name] = pd.to_numeric(blk.iloc[:, i], errors="coerce")

    return data.dropna(subset=["datetime"]).reset_index(drop=True)

def load_weather(uploaded_file) -> pd.DataFrame:
    raw = read_report_table(uploaded_file, header=None)
    blk = extract_datetime_block(raw, dt_col=0)

    data = pd.DataFrame()
    data["datetime"] = blk["_dt_parsed_"]

    cols_expected = ["inv_kw_weather", "poa_wm2", "ghi_wm2", "module_c", "ambient_c"]
    for i, name in enumerate(cols_expected, start=2):
        if i < blk.shape[1]:
            data[name] = pd.to_numeric(blk.iloc[:, i], errors="coerce")

    return data.dropna(subset=["datetime"]).reset_index(drop=True)

# =========================
# Session storage
# =========================
def init_store():
    if "store" not in st.session_state:
        st.session_state["store"] = {"mppt": {}, "inv": {}, "wea": {}}
    if "merged_df" not in st.session_state:
        st.session_state["merged_df"] = None
    # ì—…ë¡œë” ìœ„ì ¯ì„ ê°•ì œ ì´ˆê¸°í™”í•˜ê¸° ìœ„í•œ ë¦¬ì…‹ ì¹´ìš´í„°
    if "uploader_reset" not in st.session_state:
        st.session_state["uploader_reset"] = 0

def add_files(kind: str, files, loader):
    if not files:
        return
    for f in files:
        k = file_key(f)
        if k in st.session_state["store"][kind]:
            continue
        try:
            df = loader(f)
            rng = detect_datetime_range(df)
            st.session_state["store"][kind][k] = {"name": f.name, "df": df, "range": rng, "error": None}
        except Exception as e:
            st.session_state["store"][kind][k] = {"name": f.name, "df": None, "range": (None, None), "error": str(e)}

def clear_all():
    # ì„¸ì…˜ ì „ì²´ clear()ë¥¼ ì“°ë©´ ì—…ë¡œë”ê°€ ê°™ì€ keyë¡œ ë‹¤ì‹œ ì‚´ì•„ë‚˜ëŠ” ì¼€ì´ìŠ¤ê°€ ìˆì–´
    # í•„ìš”í•œ ê²ƒë§Œ ë¦¬ì…‹ + ì—…ë¡œë” keyë¥¼ ë°”ê¾¸ëŠ” ì¹´ìš´í„° ì¦ê°€ ë°©ì‹ì´ ê°€ì¥ í™•ì‹¤í•©ë‹ˆë‹¤.
    st.session_state["store"] = {"mppt": {}, "inv": {}, "wea": {}}
    st.session_state["merged_df"] = None
    st.session_state["uploader_reset"] += 1
    st.rerun()

init_store()

# =========================
# Top controls
# =========================
top_left, top_right = st.columns([2, 1])
with top_left:
    st.info("ì—¬ëŸ¬ íŒŒì¼ì„ ëˆ„ì  ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (í™”ë©´ ë°€ë¦¼ ë°©ì§€: ìƒíƒœ ëª©ë¡ì€ ì ‘ì–´ì„œ í‘œì‹œë©ë‹ˆë‹¤)")
with top_right:
    if st.button("ì „ì²´ ì´ˆê¸°í™”", type="secondary", use_container_width=True):
        clear_all()

# =========================
# Upload UI (ì—¬ëŸ¬ íŒŒì¼)
# =========================
r = st.session_state["uploader_reset"]  # ì—…ë¡œë” ê°•ì œ ì´ˆê¸°í™”ìš©

c1, c2, c3 = st.columns(3)
with c1:
    st.subheader("MPPT (ì—¬ëŸ¬ íŒŒì¼)")
    mppt_files = st.file_uploader(
        "MPPT ì—…ë¡œë“œ",
        type=["xls", "xlsx"],
        accept_multiple_files=True,
        key=f"mppt_uploader_{r}",
    )
with c2:
    st.subheader("ì¸ë²„í„° (ì—¬ëŸ¬ íŒŒì¼)")
    inv_files = st.file_uploader(
        "ì¸ë²„í„° ì—…ë¡œë“œ",
        type=["xls", "xlsx"],
        accept_multiple_files=True,
        key=f"inv_uploader_{r}",
    )
with c3:
    st.subheader("ê¸°ìƒ (ì—¬ëŸ¬ íŒŒì¼)")
    wea_files = st.file_uploader(
        "ê¸°ìƒ ì—…ë¡œë“œ",
        type=["xls", "xlsx"],
        accept_multiple_files=True,
        key=f"wea_uploader_{r}",
    )

add_files("mppt", mppt_files, load_mppt)
add_files("inv", inv_files, load_inverter)
add_files("wea", wea_files, load_weather)

# =========================
# Compact summary (ìœ„ì—ì„œ ë°”ë¡œ í™•ì¸)
# =========================
st.divider()
st.subheader("ìš”ì•½")
mppt_n = len(st.session_state["store"]["mppt"])
inv_n  = len(st.session_state["store"]["inv"])
wea_n  = len(st.session_state["store"]["wea"])
st.write(f"- MPPT íŒŒì¼: **{mppt_n}ê°œ** | ì¸ë²„í„° íŒŒì¼: **{inv_n}ê°œ** | ê¸°ìƒ íŒŒì¼: **{wea_n}ê°œ**")

# =========================
# Status (ì ‘ê¸°: í™”ë©´ ë°€ë¦¼ ë°©ì§€)
# =========================
with st.expander("ì—…ë¡œë“œ/ì¸ì‹ ìƒíƒœ ë³´ê¸°", expanded=False):
    st.caption("íŒŒì¼ì´ ë§ì•„ë„ í™”ë©´ì´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ ê¸°ë³¸ì€ ì ‘í˜€ ìˆìŠµë‹ˆë‹¤.")

    def render_store(kind: str, label: str):
        items = st.session_state["store"][kind]
        st.markdown(f"### {label} ({len(items)}ê°œ)")
        if not items:
            st.info("ì•„ì§ ì—†ìŒ")
            return
        for v in items.values():
            if v["error"]:
                st.error(f"- {v['name']} | ì¸ì‹ ì‹¤íŒ¨: {v['error']}")
            else:
                s, e = v["range"]
                # s/eê°€ Noneì¼ ê°€ëŠ¥ì„± ë°©ì–´
                if s is None or e is None:
                    st.success(f"- {v['name']} | ê¸°ê°„: (ì•Œ ìˆ˜ ì—†ìŒ) | rows={len(v['df'])}")
                else:
                    st.success(f"- {v['name']} | ê¸°ê°„: {s:%Y-%m-%d %H:%M} ~ {e:%Y-%m-%d %H:%M} | rows={len(v['df'])}")

    s1, s2, s3 = st.columns(3)
    with s1: render_store("mppt", "MPPT")
    with s2: render_store("inv", "ì¸ë²„í„°")
    with s3: render_store("wea", "ê¸°ìƒ")

# =========================
# Merge
# =========================
st.divider()
st.subheader("í•©ì„±")

def build_merged_hourly():
    def concat_kind(kind: str):
        frames = []
        for v in st.session_state["store"][kind].values():
            if v["df"] is not None and v["error"] is None:
                frames.append(v["df"])
        if not frames:
            return None
        out = pd.concat(frames, ignore_index=True)
        out = dedup_on_datetime(out)
        out = resample_hourly_mean(out)  # ì‹œê°„ ë‹¨ìœ„ ê³ ì •
        out = dedup_on_datetime(out)     # ì•ˆì „
        return out

    mppt = concat_kind("mppt")
    inv  = concat_kind("inv")
    wea  = concat_kind("wea")

    mppt = prefix_df(mppt, "mppt_")
    inv  = prefix_df(inv,  "inv_")
    wea  = prefix_df(wea,  "wea_")

    dfs = [d for d in [mppt, inv, wea] if d is not None]
    if not dfs:
        return None

    out = dfs[0]
    for d in dfs[1:]:
        out = out.merge(d, on="datetime", how="outer")

    out = out.sort_values("datetime").reset_index(drop=True)
    return out

b1, b2 = st.columns([1, 1])
with b1:
    if st.button("í•©ì„± ì‹¤í–‰", type="primary", use_container_width=True):
        merged = build_merged_hourly()
        st.session_state["merged_df"] = merged
        if merged is None or merged.empty:
            st.error("í•©ì„±í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        else:
            s, e = detect_datetime_range(merged)
            if s is None or e is None:
                st.success(f"í•©ì„± ì™„ë£Œ | rows={len(merged)}")
            else:
                st.success(f"í•©ì„± ì™„ë£Œ | ê¸°ê°„: {s:%Y-%m-%d %H:%M} ~ {e:%Y-%m-%d %H:%M} | rows={len(merged)}")

with b2:
    if st.button("ê³µë°± ê¸°ê°„ 0 ì¶”ê°€", use_container_width=True):
        merged = st.session_state.get("merged_df")
        if merged is None or merged.empty:
            st.warning("ë¨¼ì € [í•©ì„± ì‹¤í–‰]ìœ¼ë¡œ í†µí•©ë³¸ì„ ë§Œë“  ë’¤ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        else:
            filled = fill_missing_hours_and_zero(merged)
            st.session_state["merged_df"] = filled
            s, e = detect_datetime_range(filled)
            if s is None or e is None:
                st.success(f"ê³µë°± ì‹œê°„í–‰ ìƒì„± + ê²°ì¸¡ 0 ì±„ì›€ ì™„ë£Œ | rows={len(filled)}")
            else:
                st.success(f"ê³µë°± ì‹œê°„í–‰ ìƒì„± + ê²°ì¸¡ 0 ì±„ì›€ ì™„ë£Œ | ê¸°ê°„: {s:%Y-%m-%d %H:%M} ~ {e:%Y-%m-%d %H:%M} | rows={len(filled)}")

# =========================
# Download merged
# =========================
st.divider()
st.subheader("í†µí•©ë³¸ ë‹¤ìš´ë¡œë“œ")

merged_df = st.session_state.get("merged_df")
if merged_df is None or merged_df.empty:
    st.info("ì•„ì§ í•©ì„±ëœ í†µí•©ë³¸ì´ ì—†ìŠµë‹ˆë‹¤. [í•©ì„± ì‹¤í–‰]ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
else:
    s, e = detect_datetime_range(merged_df)
    if s is not None and e is not None:
        st.write(f"- í†µí•© ê¸°ê°„: {s:%Y-%m-%d %H:%M} ~ {e:%Y-%m-%d %H:%M}")
    st.write(f"- í–‰ ìˆ˜: {len(merged_df)} / ì»¬ëŸ¼ ìˆ˜: {merged_df.shape[1]}")

    st.download_button(
        "í†µí•©ë³¸ ë‹¤ìš´ë¡œë“œ(.xlsx)",
        data=to_excel_bytes(merged_df, "Merged"),
        file_name="í†µí•©_ì •ë¦¬ë³¸.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        use_container_width=True
    )

