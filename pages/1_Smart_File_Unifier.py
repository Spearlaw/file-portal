import streamlit as st

def check_password():
    if "authenticated" not in st.session_state:
        st.session_state.authenticated = False

    if not st.session_state.authenticated:
        st.title("ğŸ”’ ë©ì‹¤ ì „ìš© í˜ì´ì§€")
        pwd = st.text_input("ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”", type="password")

        if pwd:
            if pwd == st.secrets["APP_PASSWORD"]:
                st.session_state.authenticated = True
                st.rerun()
            else:
                st.error("ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
        return False

    return True


if not check_password():
    st.stop()

import streamlit as st
import pandas as pd
import io
from datetime import datetime, time
import re
import csv
from typing import Optional, Tuple, List, Any, Dict

st.title("ğŸš€ Smart File Unifier")

# =============================
# (ì¶”ê°€) 1ë¶„ ëŒ€ìš©ëŸ‰ ë³´í˜¸ ì„¤ì •
# =============================
# - 1ë¶„ + ê³µë°±ì±„ìš°ê¸°(reindex) ì‹œ í–‰ ìˆ˜ê°€ í­ì¦í•˜ë©´ ê³µìœ ì„œë²„ì—ì„œ íŠ•ê¸¸ í™•ë¥ ì´ í½ë‹ˆë‹¤.
# - ì•„ë˜ ì„ê³„ê°’ì€ ë³´ìˆ˜ì ìœ¼ë¡œ ì¡ì•˜ìŠµë‹ˆë‹¤. í•„ìš”í•˜ë©´ ì¡°ì ˆí•˜ì„¸ìš”.
FILL_ROW_LIMIT_1MIN = 200_000          # 1ë¶„ ê³µë°±ì±„ìš°ê¸° í—ˆìš© ìµœëŒ€ í–‰ìˆ˜ (ì•½ 139ì¼ ë¶„ëŸ‰)
XLSX_ROW_LIMIT_WARN = 150_000          # ì—‘ì…€ ì €ì¥ì´ ìœ„í—˜í•´ì§€ê¸° ì‹œì‘í•˜ëŠ” í–‰ìˆ˜(ê²½ê³ /CSV ê¶Œì¥)

# -----------------------------
# uploader reset key / confirm
# -----------------------------
if "uploader_key" not in st.session_state:
    st.session_state["uploader_key"] = 0
if "confirm_reset" not in st.session_state:
    st.session_state["confirm_reset"] = False

def safe_rerun():
    if hasattr(st, "rerun"):
        st.rerun()
    elif hasattr(st, "experimental_rerun"):
        st.experimental_rerun()
    return

# -----------------------------
# ì „ì²´ ì œê±° ë²„íŠ¼(í™•ì¸ í¬í•¨)
# -----------------------------
cbtn, _ = st.columns([1, 6])
with cbtn:
    if st.button("ğŸ—‘ ì „ì²´ ì œê±°"):
        st.session_state["confirm_reset"] = True

if st.session_state["confirm_reset"]:
    st.warning("ì—…ë¡œë“œëœ íŒŒì¼ê³¼ í˜„ì¬ ë¶„ì„ ê²°ê³¼ê°€ ëª¨ë‘ ì œê±°ë©ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
    y, n = st.columns([1, 1])
    with y:
        if st.button("âœ… ì˜ˆ, ëª¨ë‘ ì œê±°"):
            st.session_state["uploader_key"] += 1
            st.session_state.pop("combined_df", None)
            st.session_state.pop("filtered_df", None)
            st.session_state.pop("upload_signature", None)
            st.session_state["confirm_reset"] = False
            safe_rerun()
    with n:
        if st.button("âŒ ì·¨ì†Œ"):
            st.session_state["confirm_reset"] = False

uploaded_files = st.file_uploader(
    "íŒŒì¼ì„ í•œêº¼ë²ˆì— ì—…ë¡œë“œí•˜ì„¸ìš”",
    accept_multiple_files=True,
    key=f"uploader_{st.session_state['uploader_key']}"
)

# -----------------------------
# time parse
# -----------------------------
def parse_hhmm(s: str, *, allow_2400: bool = False):
    if s is None:
        return None, "ì‹œê°„ ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤."
    s = s.strip()
    if not re.fullmatch(r"\d{1,2}:\d{2}", s):
        return None, "í˜•ì‹ ì˜¤ë¥˜: HH:MM í˜•íƒœë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”. ì˜ˆ) 09:30, 0:05"

    hh, mm = s.split(":")
    hh = int(hh); mm = int(mm)

    if mm < 0 or mm > 59:
        return None, "ë¶„(mm)ì€ 00~59 ë²”ìœ„ì—¬ì•¼ í•©ë‹ˆë‹¤."

    if hh == 24 and mm == 0 and allow_2400:
        return time(23, 59, 59), None

    if hh < 0 or hh > 23:
        return None, "ì‹œ(HH)ëŠ” 00~23 ë²”ìœ„ì—¬ì•¼ í•©ë‹ˆë‹¤. (ì¢…ë£Œ ì‹œê°„ë§Œ 24:00 í—ˆìš©)"

    return time(hh, mm, 0), None


# -----------------------------
# robust loader (format-agnostic)
# -----------------------------
TS_CANDIDATES = [
    "TIMESTAMP", "Timestamp", "timestamp",
    "DateTime", "DATETIME", "DATE_TIME", "DATE TIME",
    "Time", "TIME", "Date", "DATE"
]

def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:
    df.columns = [str(c).strip() for c in df.columns]
    if "TIMESTAMP" not in df.columns:
        upper_map = {str(c).strip().upper(): c for c in df.columns}
        for cand in TS_CANDIDATES:
            key = cand.upper()
            if key in upper_map:
                df = df.rename(columns={upper_map[key]: "TIMESTAMP"})
                break
    return df

def decode_text_best_effort(file_bytes: bytes) -> Tuple[str, str]:
    for enc in ["utf-8-sig", "utf-8", "cp949"]:
        try:
            return file_bytes.decode(enc, errors="strict"), enc
        except Exception:
            pass
    return file_bytes.decode("utf-8", errors="replace"), "utf-8(replace)"

def sniff_delimiter(text: str) -> str:
    sample = "\n".join(text.splitlines()[:80])
    try:
        d = csv.Sniffer().sniff(sample, delimiters=[",", "\t", ";", "|"])
        return d.delimiter
    except Exception:
        if "\t" in sample: return "\t"
        if ";" in sample: return ";"
        if "|" in sample: return "|"
        return ","

def find_header_line_index(text: str, max_lines: int = 300) -> Optional[int]:
    lines = text.splitlines()
    upper_candidates = [c.upper() for c in TS_CANDIDATES]
    for i, line in enumerate(lines[:max_lines]):
        u = line.upper()
        if any(c in u for c in upper_candidates):
            return i
    return None

def postprocess_df(df: pd.DataFrame) -> pd.DataFrame:
    df = normalize_columns(df)
    if "TIMESTAMP" in df.columns:
        df["TIMESTAMP"] = pd.to_datetime(df["TIMESTAMP"], errors="coerce")
        df = df.dropna(subset=["TIMESTAMP"])
    return df

def score_timestamp_quality(df: Optional[pd.DataFrame]) -> int:
    if df is None or df.empty: return -10**9
    if "TIMESTAMP" not in df.columns: return -10**9
    ts = pd.to_datetime(df["TIMESTAMP"], errors="coerce")
    good = int(ts.notna().sum())
    total = int(len(ts))
    if total == 0 or good == 0: return -10**9
    ratio = good / total
    uniq = int(ts.dropna().nunique())
    mono = int(ts.dropna().is_monotonic_increasing)
    return int(ratio * 1_000_000) + good * 10 + uniq + mono * 1000

def try_read_csv_variant(file_bytes: bytes, encoding: str, delimiter: str,
                         header: Any, skiprows: Optional[List[int]] = None) -> Optional[pd.DataFrame]:
    try:
        return pd.read_csv(
            io.BytesIO(file_bytes),
            encoding=encoding,
            delimiter=delimiter,
            header=header,
            skiprows=skiprows,
            engine="python",
        )
    except Exception:
        return None

def try_read_excel_variant(file_bytes: bytes, skiprows: Optional[List[int]] = None,
                           header: Any = 0) -> Optional[pd.DataFrame]:
    try:
        return pd.read_excel(io.BytesIO(file_bytes), skiprows=skiprows, header=header)
    except Exception:
        return None

def pick_best_dataframe(candidates: List[Tuple[str, Optional[pd.DataFrame]]]) -> Optional[pd.DataFrame]:
    best_df = None
    best_score = -10**18
    for _, raw in candidates:
        if raw is None:
            continue
        df = postprocess_df(raw)
        sc = score_timestamp_quality(df)
        if sc > best_score:
            best_score = sc
            best_df = df
    return best_df

def read_any_file_from_bytes(file_bytes: bytes, filename: str) -> Tuple[Optional[pd.DataFrame], Optional[str]]:
    ext = filename.lower().split(".")[-1] if "." in filename else ""

    if ext == "xls":
        return None, "í˜„ì¬ ë²„ì „ì—ì„œëŠ” ë³´ê³ ì„œí˜• .xls ì·¨í•©ì„ ì œì™¸í–ˆìŠµë‹ˆë‹¤."

    if ext in ["xlsx", "xlsm", "xltx", "xltm"]:
        candidates: List[Tuple[str, Optional[pd.DataFrame]]] = []
        candidates.append(("xlsx_header0", try_read_excel_variant(file_bytes, skiprows=None, header=0)))
        candidates.append(("xlsx_skip_0_2_3", try_read_excel_variant(file_bytes, skiprows=[0, 2, 3], header=0)))

        head = try_read_excel_variant(file_bytes, skiprows=None, header=None)
        if head is not None and not head.empty:
            max_rows = min(300, len(head))
            header_idx = None
            for i in range(max_rows):
                row = head.iloc[i].astype(str).str.upper().tolist()
                if any(c.upper() in " ".join(row) for c in TS_CANDIDATES):
                    header_idx = i
                    break
            if header_idx is not None and header_idx > 0:
                candidates.append((f"xlsx_header_at_{header_idx}",
                                   try_read_excel_variant(file_bytes, skiprows=list(range(header_idx)), header=0)))

        best = pick_best_dataframe(candidates)
        if best is None:
            return None, "ì—‘ì…€ íŒŒì‹± ì‹¤íŒ¨: TIMESTAMPë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        return best, None

    text, enc = decode_text_best_effort(file_bytes)
    delim = sniff_delimiter(text)
    header_idx = find_header_line_index(text)

    candidates_t: List[Tuple[str, Optional[pd.DataFrame]]] = []
    candidates_t.append(("csv_header0", try_read_csv_variant(file_bytes, enc, delim, header=0, skiprows=None)))
    if header_idx is not None and header_idx > 0:
        candidates_t.append((f"csv_header_at_{header_idx}",
                             try_read_csv_variant(file_bytes, enc, delim, header=0, skiprows=list(range(header_idx)))))
    candidates_t.append(("csv_skip_0_2_3", try_read_csv_variant(file_bytes, enc, delim, header=0, skiprows=[0, 2, 3])))

    for alt in [",", "\t", ";", "|"]:
        if alt == delim:
            continue
        candidates_t.append((f"csv_header0_delim_{repr(alt)}",
                             try_read_csv_variant(file_bytes, enc, alt, header=0, skiprows=None)))
        candidates_t.append((f"csv_skip_0_2_3_delim_{repr(alt)}",
                             try_read_csv_variant(file_bytes, enc, alt, header=0, skiprows=[0, 2, 3])))
        if header_idx is not None and header_idx > 0:
            candidates_t.append((f"csv_header_at_{header_idx}_delim_{repr(alt)}",
                                 try_read_csv_variant(file_bytes, enc, alt, header=0, skiprows=list(range(header_idx)))))

    best = pick_best_dataframe(candidates_t)
    if best is None:
        return None, f"CSV/DAT íŒŒì‹± ì‹¤íŒ¨: TIMESTAMPë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (encoding={enc})"
    return best, None


# -----------------------------
# dedup / conflicts / fill
# -----------------------------
def get_value_cols(df: pd.DataFrame) -> List[str]:
    return [c for c in df.columns if c not in ("TIMESTAMP", "RECORD")]

def drop_exact_duplicates_excluding_record(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty or "TIMESTAMP" not in df.columns:
        return df
    value_cols = [c for c in get_value_cols(df) if c in df.columns]
    subset = ["TIMESTAMP"] + value_cols
    return df.drop_duplicates(subset=subset, keep="last").copy()

def resolve_timestamp_conflicts_most_non_null(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty or "TIMESTAMP" not in df.columns:
        return df
    value_cols = [c for c in get_value_cols(df) if c in df.columns]
    d = df.copy()
    d["_nn"] = d[value_cols].notna().sum(axis=1) if value_cols else 0
    d["_rowid"] = range(len(d))
    d = d.sort_values(["TIMESTAMP", "_nn", "_rowid"])
    picked = d.groupby("TIMESTAMP", as_index=False).tail(1).drop(columns=["_nn", "_rowid"])
    return picked.sort_values("TIMESTAMP").copy()

def fill_missing_by_reindex(df: pd.DataFrame, start_dt: datetime, end_dt: datetime, freq: str) -> pd.DataFrame:
    full_range = pd.date_range(start=pd.Timestamp(start_dt), end=pd.Timestamp(end_dt), freq=freq)
    if df.empty:
        return pd.DataFrame({"TIMESTAMP": full_range})
    d = df.copy().sort_values("TIMESTAMP").set_index("TIMESTAMP")
    d = d.reindex(full_range)
    d.index.name = "TIMESTAMP"
    return d.reset_index()

def looks_numeric_series(s: pd.Series, sample_n: int = 200, threshold: float = 0.85) -> bool:
    x = s.dropna()
    if x.empty:
        return False
    if len(x) > sample_n:
        x = x.sample(sample_n, random_state=1)
    if pd.api.types.is_numeric_dtype(x):
        return True
    xs = x.astype(str).str.strip()
    xs = xs[xs != ""]
    if xs.empty:
        return False
    conv = pd.to_numeric(xs, errors="coerce")
    success = conv.notna().mean()
    return (success >= threshold) and (conv.notna().sum() > 0)

def fill_zeros_for_numeric_like_columns(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df
    value_cols = [c for c in df.columns if c not in ("TIMESTAMP", "RECORD")]
    numeric_like = []
    for c in value_cols:
        try:
            if looks_numeric_series(df[c]):
                numeric_like.append(c)
        except Exception:
            continue

    out = df.copy()
    for c in numeric_like:
        out[c] = pd.to_numeric(out[c], errors="coerce").fillna(0)

    return out

# âœ… (ì¶”ê°€) 1ì¼ ê°„ê²© ì„ íƒ ì‹œ ë‚ ì§œ ê²½ê³„ë¡œ ì •ê·œí™”
def normalize_to_day_bounds(start_dt: datetime, end_dt: datetime) -> Tuple[datetime, datetime]:
    s = datetime.combine(start_dt.date(), time(0, 0, 0))
    e = datetime.combine(end_dt.date(), time(23, 59, 59))
    return s, e

# âœ… (ì¶”ê°€) ì„ íƒ ê°„ê²© ê¸°ì¤€ ì˜ˆìƒ í–‰ ìˆ˜ ê³„ì‚°(ê³µë°± ì±„ìš°ê¸° ì•ˆì „ì¥ì¹˜ìš©)
def estimate_rows(start_dt: datetime, end_dt: datetime, freq_label: str) -> int:
    seconds = (end_dt - start_dt).total_seconds()
    if seconds < 0:
        return 0
    if freq_label == "1ë¶„":
        step = 60
    elif freq_label == "10ë¶„":
        step = 600
    elif freq_label == "1ì‹œê°„":
        step = 3600
    elif freq_label == "1ì¼":
        step = 86400
    else:
        step = 60
    return int(seconds // step) + 1


# -----------------------------
# main
# -----------------------------
if uploaded_files:
    current_signature = tuple((f.name, getattr(f, "size", None)) for f in uploaded_files)

    if "combined_df" not in st.session_state or st.session_state.get("upload_signature") != current_signature:
        all_dfs: List[pd.DataFrame] = []
        failed: List[Tuple[str, str]] = []
        success: List[str] = []
        file_schema: Dict[str, List[str]] = {}

        st.write("### â³ íŒŒì¼ ë¡œë”© ì§„í–‰")

        PARSE_WEIGHT = 0.35
        sizes = [(f.name, int((getattr(f, "size", 0) or 0))) for f in uploaded_files]
        total_units = sum(int(sz * (1.0 + PARSE_WEIGHT)) for _, sz in sizes) or 1
        done_units = 0

        progress_bar = st.progress(0)
        progress_text = st.empty()

        def set_progress(note: str = ""):
            pct = int(done_units / total_units * 100)
            pct = max(0, min(100, pct))
            progress_bar.progress(pct)
            if note:
                progress_text.write(f"{pct}% - {note}")

        total = len(uploaded_files)

        for idx, f in enumerate(uploaded_files, start=1):
            sz = int((getattr(f, "size", 0) or 0))
            set_progress(f"ì²˜ë¦¬ ì¤‘: {f.name} ({idx}/{total})")

            file_bytes = f.getvalue()
            done_units += int(len(file_bytes) * 1.0)
            set_progress(f"{f.name} ì½ê¸° ì™„ë£Œ")

            df, err = read_any_file_from_bytes(file_bytes, f.name)
            done_units += int(sz * PARSE_WEIGHT)

            if err or df is None:
                failed.append((f.name, err or "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"))
            else:
                # =============================
                # (ì¶”ê°€) íŒŒì¼ ë‹¨ìœ„ ì„ ì •ë¦¬(ê¸°ëŠ¥ ìœ ì§€ + ëŒ€ìš©ëŸ‰ ì•ˆì •ì„±â†‘)
                # - ê¸°ì¡´ì— ìµœì¢… ë‹¨ê³„ì—ì„œ í•˜ë˜ ì •ë¦¬ì™€ ë™ì¼í•œ ë¡œì§ì„ "íŒŒì¼ë³„ë¡œë„" í•œ ë²ˆ ìˆ˜í–‰
                # - ìµœì¢… concat í›„ì—ë„ ê¸°ì¡´ëŒ€ë¡œ í•œ ë²ˆ ë” ìˆ˜í–‰í•˜ë¯€ë¡œ ì•ˆì „ë§ ìœ ì§€
                # =============================
                if "TIMESTAMP" in df.columns and not df.empty:
                    df = df.sort_values("TIMESTAMP").reset_index(drop=True)
                    df = drop_exact_duplicates_excluding_record(df)
                    df = resolve_timestamp_conflicts_most_non_null(df)
                    df = df.sort_values("TIMESTAMP").reset_index(drop=True)

                all_dfs.append(df)
                success.append(f.name)
                file_schema[f.name] = list(df.columns)

        done_units = total_units
        set_progress("ì™„ë£Œ")

        st.write("### âœ… ë¡œë“œ ê²°ê³¼ ìš”ì•½")
        st.write(f"- ì „ì²´: **{total}ê°œ** | ì„±ê³µ: **{len(success)}ê°œ** | ì‹¤íŒ¨: **{len(failed)}ê°œ**")

        if failed:
            with st.expander("ì‹¤íŒ¨í•œ íŒŒì¼ ë³´ê¸°(ì›ì¸ í¬í•¨)"):
                for n, e in failed:
                    st.write(f"- âŒ {n}: {e}")

        with st.expander("íŒŒì¼ë³„ ì»¬ëŸ¼(ìŠ¤í‚¤ë§ˆ) í™•ì¸"):
            for n, cols in file_schema.items():
                st.write(f"- **{n}**: {cols}")

        if all_dfs:
            combined_df = pd.concat(all_dfs, axis=0, ignore_index=True, sort=False)

            # (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€) ìµœì¢… í†µí•©ë³¸ì—ì„œë„ ë™ì¼ ì •ë¦¬ 1íšŒ ìˆ˜í–‰
            if "TIMESTAMP" in combined_df.columns:
                combined_df = combined_df.sort_values("TIMESTAMP").reset_index(drop=True)
                combined_df = drop_exact_duplicates_excluding_record(combined_df)
                combined_df = resolve_timestamp_conflicts_most_non_null(combined_df)
                combined_df = combined_df.sort_values("TIMESTAMP").reset_index(drop=True)

            st.session_state["combined_df"] = combined_df
            st.session_state["upload_signature"] = current_signature
            st.session_state.pop("filtered_df", None)
        else:
            st.session_state["combined_df"] = pd.DataFrame()
            st.session_state["upload_signature"] = current_signature
            st.session_state.pop("filtered_df", None)

    combined_df = st.session_state["combined_df"]

    if combined_df is None or len(combined_df) == 0:
        st.warning("ìœ íš¨í•˜ê²Œ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (í—¤ë”/êµ¬ë¶„ì/ì¸ì½”ë”©/TIMESTAMP ë“±ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.)")
    else:
        recognized_min = recognized_max = None
        if "TIMESTAMP" in combined_df.columns and len(combined_df) > 0:
            recognized_min = combined_df["TIMESTAMP"].min()
            recognized_max = combined_df["TIMESTAMP"].max()

        st.write("## ğŸ“Œ í†µí•© ê²°ê³¼ ìš”ì•½")
        st.write(f"- ì…ë ¥ íŒŒì¼ ê°œìˆ˜: **{len(uploaded_files)}ê°œ**")
        st.write(f"- í†µí•© í–‰ ìˆ˜(ì •ë¦¬ í›„): **{len(combined_df)}í–‰**")
        if recognized_min is not None and recognized_max is not None:
            st.write(f"- ì¸ì‹ ê¸°ê°„: **{recognized_min:%Y-%m-%d %H:%M:%S} ~ {recognized_max:%Y-%m-%d %H:%M:%S}**")
        else:
            st.warning("TIMESTAMPê°€ ì—†ì–´ ì¸ì‹ ê¸°ê°„/ê¸°ê°„ í•„í„°/ê³µë°± ì±„ìš°ê¸° ê¸°ëŠ¥ì´ ì œí•œë©ë‹ˆë‹¤.")

        if recognized_min is not None and recognized_max is not None:
            st.write("### ğŸ§­ ë°ì´í„° ì„¤ì • (ì ìš© ë²„íŠ¼ì„ ëˆŒëŸ¬ì•¼ ë°˜ì˜ë©ë‹ˆë‹¤)")
            with st.form("settings_form", clear_on_submit=False):
                c1, c2, c3, c4 = st.columns([1, 1, 1, 1])
                with c1:
                    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", value=recognized_min.date())
                with c2:
                    start_time_str = st.text_input("ì‹œì‘ ì‹œê°„(HH:MM)", value=recognized_min.strftime("%H:%M"))
                with c3:
                    end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", value=recognized_max.date())
                with c4:
                    end_time_str = st.text_input("ì¢…ë£Œ ì‹œê°„(HH:MM)", value=recognized_max.strftime("%H:%M"))

                st.write("#### ğŸ§© ì‹œê³„ì—´ ê³µë°± 0 ì±„ìš°ê¸°")
                freq_map = {"1ë¶„": "1T", "10ë¶„": "10T", "1ì‹œê°„": "1H", "1ì¼": "1D"}
                freq_label = st.selectbox("ë°ì´í„° ê°„ê²©(ê³µë°± ì±„ìš°ê¸° ê¸°ì¤€)", ["1ë¶„", "10ë¶„", "1ì‹œê°„", "1ì¼"], index=2)

                # =============================
                # (ì¶”ê°€) 1ë¶„ ëª¨ë“œ ì•ˆì „ì¥ì¹˜ ì•ˆë‚´
                # =============================
                if freq_label == "1ë¶„":
                    st.info(
                        f"âš ï¸ 1ë¶„ ë‹¨ìœ„ëŠ” ë°ì´í„°ê°€ ë§¤ìš° ì»¤ì§ˆ ìˆ˜ ìˆì–´ ê³µìœ  ì„œë²„ì—ì„œ íŠ•ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                        f"- ê³µë°± ì±„ìš°ê¸°(ë¦¬ì¸ë±ìŠ¤)ëŠ” ì„ íƒ ê¸°ê°„ì´ ì»¤ì§€ë©´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                        f"- ì„ê³„ê°’: ì•½ {FILL_ROW_LIMIT_1MIN:,}í–‰(1ë¶„ ê¸°ì¤€) ì´ˆê³¼ ì‹œ ì°¨ë‹¨"
                    )
                fill_missing = st.checkbox("ì„ íƒ ê¸°ê°„ ë‚´ ëˆ„ë½ëœ ì‹œê°„ì„ 0ìœ¼ë¡œ ì±„ìš°ê¸°", value=True)

                apply_btn = st.form_submit_button("âœ… ì ìš©")

            if apply_btn:
                start_time, err1 = parse_hhmm(start_time_str, allow_2400=False)
                end_time, err2 = parse_hhmm(end_time_str, allow_2400=True)
                if err1: st.error(f"ì‹œì‘ ì‹œê°„ ì˜¤ë¥˜: {err1}")
                if err2: st.error(f"ì¢…ë£Œ ì‹œê°„ ì˜¤ë¥˜: {err2}")

                if (not err1) and (not err2):
                    start_dt = datetime.combine(start_date, start_time)
                    end_dt = datetime.combine(end_date, end_time)

                    if freq_label == "1ì¼":
                        start_dt, end_dt = normalize_to_day_bounds(start_dt, end_dt)

                    if start_dt > end_dt:
                        st.error("ê¸°ê°„ ì„ íƒ ì˜¤ë¥˜: ì‹œì‘ì´ ì¢…ë£Œë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤.")
                    else:
                        filtered_df = combined_df[
                            (combined_df["TIMESTAMP"] >= pd.Timestamp(start_dt)) &
                            (combined_df["TIMESTAMP"] <= pd.Timestamp(end_dt))
                        ].copy()

                        # (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€) ê¸°ê°„ í•„í„° í›„ ì •ë¦¬
                        filtered_df = drop_exact_duplicates_excluding_record(filtered_df)
                        filtered_df = resolve_timestamp_conflicts_most_non_null(filtered_df)

                        st.write(f"- ì„ íƒ ê¸°ê°„: **{start_dt:%Y-%m-%d %H:%M:%S} ~ {end_dt:%Y-%m-%d %H:%M:%S}**")
                        st.write(f"- ì„ íƒ ê¸°ê°„ ë‚´ ì‹¤ì œ ë°ì´í„° í–‰ ìˆ˜(ì •ë¦¬ í›„): **{len(filtered_df)}í–‰**")

                        if fill_missing:
                            # =============================
                            # (ì¶”ê°€) 1ë¶„ ê³µë°±ì±„ìš°ê¸° ë³´í˜¸ì¥ì¹˜
                            # =============================
                            est = estimate_rows(start_dt, end_dt, freq_label)
                            if freq_label == "1ë¶„" and est > FILL_ROW_LIMIT_1MIN:
                                st.error(
                                    f"1ë¶„ ë‹¨ìœ„ ê³µë°± ì±„ìš°ê¸°ëŠ” ì„ íƒ ê¸°ê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤.\n"
                                    f"- ì˜ˆìƒ í–‰ ìˆ˜: {est:,}í–‰\n"
                                    f"- í—ˆìš© í•œë„: {FILL_ROW_LIMIT_1MIN:,}í–‰\n"
                                    f"ê¸°ê°„ì„ ì¤„ì´ê±°ë‚˜, ê³µë°± ì±„ìš°ê¸°ë¥¼ ë„ê³  ì§„í–‰í•´ ì£¼ì„¸ìš”."
                                )
                                # ê³µë°± ì±„ìš°ê¸°ë§Œ ìŠ¤í‚µí•˜ê³  ê²°ê³¼ ì €ì¥ì€ ì§„í–‰
                                st.session_state["filtered_df"] = filtered_df
                            else:
                                freq = freq_map[freq_label]
                                filled_df = fill_missing_by_reindex(filtered_df, start_dt, end_dt, freq)
                                filled_df = fill_zeros_for_numeric_like_columns(filled_df)
                                filtered_df = filled_df
                                st.success(f"ê³µë°±ì„ 0ìœ¼ë¡œ ì±„ì› ìŠµë‹ˆë‹¤. (ê°„ê²©: {freq_label})")
                                st.session_state["filtered_df"] = filtered_df
                        else:
                            st.session_state["filtered_df"] = filtered_df

        display_df = st.session_state.get("filtered_df", combined_df)

        st.write("### ğŸ“Š í†µí•© ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        st.dataframe(display_df.head(10), use_container_width=True)

        st.write("### ğŸ“¥ ë‹¤ìš´ë¡œë“œ")

        # =============================
        # (ì¶”ê°€) ëŒ€ìš©ëŸ‰ ë‹¤ìš´ë¡œë“œ ì•ˆì „ì¥ì¹˜(ê¸°ì¡´ Excel ìœ ì§€ + CSV ì˜µì…˜ ì¶”ê°€)
        # =============================
        row_cnt = int(len(display_df))
        if row_cnt >= XLSX_ROW_LIMIT_WARN:
            st.warning(
                f"í˜„ì¬ ë°ì´í„°ê°€ {row_cnt:,}í–‰ì…ë‹ˆë‹¤. ê³µìœ  ì„œë²„ì—ì„œ Excel(.xlsx) ìƒì„± ì¤‘ íŠ•ê¸¸ ìˆ˜ ìˆì–´ CSV ë‹¤ìš´ë¡œë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤."
            )

        download_fmt = st.radio(
            "ë‹¤ìš´ë¡œë“œ í˜•ì‹",
            options=["Excel(.xlsx)", "CSV(.csv)"],
            index=0 if row_cnt < XLSX_ROW_LIMIT_WARN else 1,
            horizontal=True
        )

        default_base = "Merged_Data_Output"
        file_name_input = st.text_input("ì €ì¥ íŒŒì¼ëª…(í™•ì¥ì ì œì™¸)", value=default_base).strip()
        if not file_name_input:
            file_name_input = default_base

        if download_fmt == "CSV(.csv)":
            csv_bytes = display_df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                label="ğŸ“¥ í†µí•© ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSV)",
                data=csv_bytes,
                file_name=f"{file_name_input}.csv",
                mime="text/csv"
            )
        else:
            # Excel ë‹¤ìš´ë¡œë“œ(ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
            output = io.BytesIO()
            try:
                with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
                    display_df.to_excel(writer, index=False)
                st.download_button(
                    label="ğŸ“¥ í†µí•© ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Excel)",
                    data=output.getvalue(),
                    file_name=f"{file_name_input}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            except Exception as e:
                st.error("Excel íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. CSVë¡œ ë‹¤ìš´ë¡œë“œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
                st.exception(e)
                csv_bytes = display_df.to_csv(index=False).encode("utf-8-sig")
                st.download_button(
                    label="ğŸ“¥ í†µí•© ë°ì´í„° ë‹¤ìš´ë¡œë“œ (CSVë¡œ ëŒ€ì²´)",
                    data=csv_bytes,
                    file_name=f"{file_name_input}.csv",
                    mime="text/csv"
                )
